Chiy0 üëß
Chiy0 is a comprehensive LLM assistant pipeline capable of autonomous data gathering, RAG (Retrieval-Augmented Generation), and self-supervised fine-tuning.

Named after the prodigy Chiyo-chan from Azumanga Daioh, this tool helps you "study" by scraping the web for industry-specific PDFs, organizing them into a knowledge base, and using a "Teacher" model to train a smaller, faster "Student" model on that specific domain.

üöÄ Features
Autonomous Data Collection: Uses an LLM to generate search queries and the Serper API to scrape and download relevant PDFs.

RAG Engine: Indexes documents using ChromaDB and Ollama embeddings for context-aware retrieval.

Knowledge Distillation Pipeline:

Ingestion: Converts PDFs using Docling.

Synthetic Data Generation: A "Teacher" model (Llama 3.1 8B) analyzes text chunks and creates Q&A pairs.

Fine-Tuning: A "Student" model (Llama 3.2 3B) is fine-tuned on this synthetic data using Unsloth (LoRA).

Interactive Chat: Query your newly fine-tuned model with RAG support.

üõ†Ô∏è Installation
1. Prerequisites
Python 3.10+

CUDA-enabled GPU (required for Unsloth/Bitandbytes)

Ollama installed and running.

2. Clone and Install
Clone the repository and install the required dependencies:

Bash
pip install -r requirements.txt
Note: You must have the nomic-embed-text model pulled in Ollama for embeddings to work:

Bash
ollama pull nomic-embed-text
3. Environment Setup
You need a Serper API key to allow Chiy0 to search the web for PDFs.

Get a free API Key at serper.dev.

Create a file named .env in the root directory.

Add your key to the file:

Code snippet
SERPER_API_KEY=your_api_key_here
üìñ How to Use
The project is controlled via a central hub. Run the main script to start:

Bash
python main.py
You will be presented with the Local AI Hub Menu. Here is the recommended workflow:

Step 1: Download Data (Option 1)
Select Option 1.

The system will ask what industry you are interested in (e.g., "Cybersecurity", "Cooking", "Marine Biology").

It uses an LLM to generate a smart search query.

It hits the Serper API, finds relevant PDFs, and downloads them to finetune_pdfs/.

Step 2: Index Data (Option 2)
Select Option 2.

This reads the PDFs from Step 1.

It splits them into chunks and embeds them into the Chroma vector database.

Required for RAG to work.

Step 3: Train the Model (Option 4)
Select Option 4. This is the core "Study" phase:

Ingestion: Reads the PDFs.

Teacher Generation: The larger model (Llama 3.1) reads your PDFs and generates "Question & Answer" pairs, grading them for quality.

Student Training: The smaller model (Llama 3.2) is fine-tuned using Unsloth on the high-quality Q&A pairs generated by the teacher.

The new model is saved to ./lora_model.

Step 4: Chat (Option 5)
Select Option 5.

This loads your fine-tuned student model.

It combines it with the RAG database.

You can now ask questions, and the model will answer using both its new training and the specific context from the PDFs.

üìÇ Project Structure
main.py: The CLI entry point for the application.

PDFDownloader.py: Handles Serper API communication and file downloads.

handle_database.py: Manages PDF processing, chunking, and ChromaDB indexing.

Finetuner.py: Contains the KnowledgeDistiller class for synthetic data generation and Unsloth training.

query_data.py: The RAG inference engine used during chat.

embedding_function.py: Configuration for Ollama embeddings.

‚ö†Ô∏è Troubleshooting
Ollama Connection Refused: Ensure ollama serve is running in a separate terminal.

CUDA/Memory Errors: This pipeline requires a GPU. If you run out of VRAM during training (Option 4), try reducing the batch_size in Finetuner.py.

Serper Error: Ensure your API key is correct in the .env file and you have credits available.
